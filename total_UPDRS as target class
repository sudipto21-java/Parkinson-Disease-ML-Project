# When total_UPDRS is used as a target class

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import norm
import seaborn as sb
from sklearn.metrics import classification_report,confusion_matrix,accuracy_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split

data=pd.read_csv('D:/tele.csv')
data.columns

# Data preprocessing
# Using standard deviation to remove outliers
upper_limit=data.age.median()+3*data.age.std()
lower_limit=data.age.median()-3*data.age.std()

upper=data.total_UPDRS.median()+3*data.total_UPDRS.std()
lower=data.total_UPDRS.median()-3*data.total_UPDRS.std()


data_noOutlier=data[(data.age<upper_limit)&(data.age>lower_limit)&(data.total_UPDRS<upper)&(data.total_UPDRS>lower)]
data_noOutlier

print(data.total_UPDRS.median())

test_array=data_noOutlier['total_UPDRS'].to_numpy(dtype='float')
y=test_array
y=y.astype('int')
y
train_array=data_noOutlier[['motor_UPDRS','Jitter(%)', 'Jitter(Abs)', 'Jitter:RAP', 'Jitter:PPQ5', 'Jitter:DDP',
       'Shimmer', 'Shimmer(dB)', 'Shimmer:APQ3', 'Shimmer:APQ5',
       'Shimmer:APQ11', 'Shimmer:DDA', 'NHR', 'HNR', 'RPDE', 'DFA', 'PPE']].to_numpy(dtype='float')
X=train_array
X


# Splitting the data into testing and training classes
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.30, random_state=42)


# Applying algorithms
# Gaussian Naive Bayes
from sklearn.naive_bayes import GaussianNB
model=GaussianNB()
model.fit(X_train,y_train)
y_pred=model.predict(X_test)
df1=pd.DataFrame({'Actual Status':y_test,'Predicted Status':y_pred})
df1

acc1=[]
acc=accuracy_score(y_test,y_pred)*100
acc1.append(acc)
print(acc)


# Simple Decision Tree Classifier
from sklearn import tree
dt_clf=tree.DecisionTreeClassifier(max_depth=5)
dt_clf.fit(X_train,y_train)
y_pred1=dt_clf.predict(X_test)
dt=dt_clf.score(X_train,y_train)#accuracy of model against traing data
print(accuracy_score(y_test,y_pred1)*100)
acc=accuracy_score(y_test,y_pred1)*100
acc1.append(acc)
print(acc)


# Random Forest Classifier
from sklearn import ensemble
rt_clf=ensemble.RandomForestClassifier(n_estimators=100)
rt_clf.fit(X_train,y_train)
y_pred=rt_clf.predict(X_test)
rt=rt_clf.score(X_train,y_train)

print(accuracy_score(y_test,y_pred)*100)
acc=accuracy_score(y_test,y_pred)*100
acc1.append(acc)
print(y_pred)


# Stochastic Gradient Descent Classifier
from sklearn.linear_model import SGDClassifier
sg_clf=SGDClassifier(loss="hinge", penalty="l2", max_iter=250)
sg_clf.fit(X, y)
y_pred=sg_clf.predict(X_test)
sg=sg_clf.score(X_train,y_train)

print(accuracy_score(y_test,y_pred)*100)
acc=accuracy_score(y_test,y_pred)*100
acc1.append(acc)
print(acc)


# Quadratic discrement analysis
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
import numpy as np
qd_clf = QuadraticDiscriminantAnalysis()
qd_clf.fit(X_train,y_train)
y_pred=qd_clf.predict(X_test)
qd=qd_clf.score(X_train,y_train)

print(accuracy_score(y_test,y_pred)*100)
acc=accuracy_score(y_test,y_pred)*100
acc1.append(acc)
print(acc)


# AdaBoost Classifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.datasets import make_classification
ad_clf=AdaBoostClassifier(n_estimators=100, learning_rate=1)
model=ad_clf.fit(X_train, y_train)
y_pred2=model.predict(X_test)
y_pred2

acc=accuracy_score(y_test,y_pred)*100
acc1.append(acc)
print(acc)


# Support Verctor classifier
from sklearn import svm
C=1.0
classifier=svm.SVC(kernel='linear', C=C).fit(X, y)
classifier.fit(X_train,y_train)
y_predict=classifier.predict(X_test)
print("Accuracy: ",(accuracy_score(y_test,y_predict)*100))
acc=accuracy_score(y_test,y_predict)*100
acc1.append(acc)
print(acc)


# Logistic Regression
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression
rfe = LogisticRegression()
rfe = rfe.fit(X_train,y_train)
y_predict=rfe.predict(X_test)
print("Accuracy: ",(accuracy_score(y_test,y_predict)*100))
acc=accuracy_score(y_test,y_predict)*100
acc1.append(acc)
print(acc)


# Categorising the results in mild and severe categories
test_array=y_pred
mild=0
sev=0
for i in range(len(test_array)):
    if(test_array[i]<=25):
        mild+=1
    else:
        sev+=1
sever=[mild,sev]
sever


# Plotting bar graph
from matplotlib import pyplot as plt
fig = plt.figure(figsize =(10, 7))
x=["GaussianNB","DecisionTree","RandomForest","SGD",
   "Quadratic","AdaBoost","svm","LogisticRegression"]
plt.bar(x,acc1,width=0.2,color="red")


plt.xlabel("Classifier")
plt.ylabel("accuracy")
plt.show()


# Plotting pie chart
import matplotlib.pyplot as plt
import numpy as np
mylabels = ["Mild", "Severe"]
mycolors = ["yellow","red"] 
plt.pie(sever,labels = mylabels,colors = mycolors)
plt.show() 


# Printing accuracy
print(acc1)
